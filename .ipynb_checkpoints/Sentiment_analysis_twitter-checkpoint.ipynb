{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xtotZDPUV5DN"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/2477_4140_bundle_archive.zip\", 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7SpXhc7hVpdA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "import pickle\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "MzmsiVURaZIe",
    "outputId": "6822aec5-ee5e-4e7f-8f37-64b5ea52003a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "j1x3vCnAYHYM",
    "outputId": "fbda0679-7e11-4f6b-8d68-286f47532235"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv', encoding =DATASET_ENCODING , names=DATASET_COLUMNS)\n",
    "data.head()\n",
    "X = data.iloc[:,[5]]\n",
    "Y = data.iloc[:,0]\n",
    "Y[Y == 4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "L7lCmXGkYHUO",
    "outputId": "effd9d58-5727-47fa-bd75-306e5875906f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  ...                                               text\n",
       "0       0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  ...  is upset that he can't update his Facebook by ...\n",
       "2       0  ...  @Kenichan I dived many times for the ball. Man...\n",
       "3       0  ...    my whole body feels itchy and like its on fire \n",
       "4       0  ...  @nationwideclass no, it's not behaving at all....\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "eD5FoI2lhx8K",
    "outputId": "988627d6-d090-4dce-e988-c809ab6864ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "KZTuUwFRsxGt",
    "outputId": "c0db4202-c215-42cb-9a5c-9694c86429ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8rgUdMj4YHSY",
    "outputId": "d9aa05c8-9025-4533-b491-865ab623c51d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Text-preprocessing\n",
    "\n",
    "# Missing Values\n",
    "num_missing_desc = data.isnull().sum()[2]    # No. of values with msising descriptions\n",
    "print('Number of missing values: ' + str(num_missing_desc))\n",
    "data = data.dropna()\n",
    "\n",
    "TAG_CLEANING_RE = \"@\\S+\"\n",
    "# Remove @tags\n",
    "X['text'] = X['text'].map(lambda x: re.sub(TAG_CLEANING_RE, ' ', str(x)))\n",
    "\n",
    "# Smart lowercase\n",
    "X['text'] = X['text'].map(lambda x: x.lower())\n",
    "\n",
    "# Remove numbers\n",
    "X['text'] = X['text'].map(lambda x: re.sub(r'\\d+', ' ', x))\n",
    "\n",
    "# Remove links\n",
    "TEXT_CLEANING_RE = \"https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "X['text'] = X['text'].map(lambda x: re.sub(TEXT_CLEANING_RE, ' ', x))\n",
    "\n",
    "# Remove Punctuation\n",
    "X['text']  = X['text'].map(lambda x: x.translate(x.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# Remove white spaces\n",
    "X['text'] = X['text'].map(lambda x: x.strip())\n",
    "\n",
    "# Tokenize into words\n",
    "X['text'] = X['text'].map(lambda x: word_tokenize(x))\n",
    " \n",
    "# Remove non alphabetic tokens\n",
    "X['text'] = X['text'].map(lambda x: [word for word in x if word.isalpha()])\n",
    "\n",
    "# Filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "X['text'] = X['text'].map(lambda x: [w for w in x if not w in stop_words])\n",
    "    \n",
    "# Word Lemmatization\n",
    "lem = WordNetLemmatizer()\n",
    "X['text'] = X['text'].map(lambda x: [lem.lemmatize(word,\"v\") for word in x])\n",
    "\n",
    "# Turn lists back to string\n",
    "X['text'] = X['text'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "liNeL-w5YHQO",
    "outputId": "31261a45-3502-4a3c-b7da-5100e7475483"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zl awww bummer shoulda get david carr third day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>upset update facebook texting might cry result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dive many time ball manage save rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>behave mad see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0    zl awww bummer shoulda get david carr third day\n",
       "1  upset update facebook texting might cry result...\n",
       "2      dive many time ball manage save rest go bound\n",
       "3                    whole body feel itchy like fire\n",
       "4                                     behave mad see"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "LHySiPNEYG-v",
    "outputId": "d73c7fee-67fe-49ba-f1be-ac8bcdaba533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN size: 1280000\n",
      "TEST size: 1280000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print(\"TRAIN size:\", len(X_train))\n",
    "print(\"TEST size:\", len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hzxfC4hHYuN1"
   },
   "outputs": [],
   "source": [
    "# WORD2VEC \n",
    "W2V_SIZE = 300\n",
    "W2V_WINDOW = 7\n",
    "W2V_EPOCH = 32\n",
    "W2V_MIN_COUNT = 10\n",
    "\n",
    "documents = [_text.split() for _text in X_train.text] \n",
    "w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, \n",
    "                                            window=W2V_WINDOW, \n",
    "                                            min_count=W2V_MIN_COUNT, \n",
    "                                            workers=8)\n",
    "w2v_model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w-NcaEXbYuMT",
    "outputId": "2c74653e-21c7-4548-8eba-3a6ce0594a92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 25276\n"
     ]
    }
   ],
   "source": [
    "words = w2v_model.wv.vocab.keys()\n",
    "vocab_size = len(words)\n",
    "print(\"Vocab size\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FV_YXOoouViN",
    "outputId": "cc9d89cd-4eb5-4ad3-b371-30cff54a1cfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(251363731, 289225504)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Word Embeddings\n",
    "w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "KXOzYOpMY4Ca",
    "outputId": "0bb1b574-d989-4fee-93af-d9a14a8b2abd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('suck', 0.5383214950561523),\n",
       " ('stupid', 0.4985460340976715),\n",
       " ('hat', 0.4723867177963257),\n",
       " ('ugh', 0.46225279569625854),\n",
       " ('dislike', 0.4461487829685211),\n",
       " ('despise', 0.43037712574005127),\n",
       " ('annoy', 0.4119408428668976),\n",
       " ('horrible', 0.4112866520881653),\n",
       " ('fuck', 0.40925276279449463),\n",
       " ('grr', 0.3940410614013672)]"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test word embeddings\n",
    "w2v_model.most_similar(\"hate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "viu6wTKSY36o",
    "outputId": "edd54a5b-f6de-42aa-f244-206230735b6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 232838 unique tokens.\n",
      "Shape of data tensor: (1280000, 300)\n"
     ]
    }
   ],
   "source": [
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train.text)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# Convert the data to padded sequences\n",
    "X_train_padded = tokenizer.texts_to_sequences(X_train.text)\n",
    "X_train_padded = pad_sequences(X_train_padded, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X_train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tP7UzqdKY33J"
   },
   "outputs": [],
   "source": [
    "# saving\n",
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sPQ21bg2ZJQ3",
    "outputId": "baa83f4d-de96-42d2-ccd9-9279e6dbb225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232839, 300)\n"
     ]
    }
   ],
   "source": [
    "# Embedding matrix for the embedding layer\n",
    "embedding_matrix = np.zeros((vocab_size+1, W2V_SIZE))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "mTCEC4NoZI9C",
    "outputId": "cd7c6c43-cc1c-4fc1-bd19-660df61cc4b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 300, 300)          69851700  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 70,012,201\n",
      "Trainable params: 160,501\n",
      "Non-trainable params: 69,851,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build Model \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size+1, W2V_SIZE, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8aDHAQouZUW3"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "DD75yElsZUAY",
    "outputId": "01eeb652-7975-4b60-e77b-1aee2cf28140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1152000 samples, validate on 128000 samples\n",
      "Epoch 1/15\n",
      "1152000/1152000 [==============================] - 1135s 986us/step - loss: 0.5077 - accuracy: 0.7475 - val_loss: 0.4692 - val_accuracy: 0.7748\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152000/1152000 [==============================] - 1114s 967us/step - loss: 0.4890 - accuracy: 0.7602 - val_loss: 0.4644 - val_accuracy: 0.7789\n",
      "Epoch 3/15\n",
      "1152000/1152000 [==============================] - 1113s 966us/step - loss: 0.4836 - accuracy: 0.7641 - val_loss: 0.4625 - val_accuracy: 0.7783\n",
      "Epoch 4/15\n",
      "1152000/1152000 [==============================] - 1104s 959us/step - loss: 0.4804 - accuracy: 0.7660 - val_loss: 0.4588 - val_accuracy: 0.7817\n",
      "Epoch 5/15\n",
      "1152000/1152000 [==============================] - 1103s 957us/step - loss: 0.4778 - accuracy: 0.7675 - val_loss: 0.4581 - val_accuracy: 0.7825\n",
      "Epoch 6/15\n",
      "1152000/1152000 [==============================] - 1126s 977us/step - loss: 0.4770 - accuracy: 0.7682 - val_loss: 0.4573 - val_accuracy: 0.7818\n",
      "Epoch 7/15\n",
      "1152000/1152000 [==============================] - 1121s 973us/step - loss: 0.4780 - accuracy: 0.7675 - val_loss: 0.4580 - val_accuracy: 0.7825\n",
      "Epoch 8/15\n",
      "1152000/1152000 [==============================] - 1211s 1ms/step - loss: 0.4799 - accuracy: 0.7664 - val_loss: 0.4669 - val_accuracy: 0.7780\n",
      "Epoch 9/15\n",
      "1152000/1152000 [==============================] - 1258s 1ms/step - loss: 0.4822 - accuracy: 0.7654 - val_loss: 0.4596 - val_accuracy: 0.7812\n",
      "Epoch 10/15\n",
      "1152000/1152000 [==============================] - 1254s 1ms/step - loss: 0.4786 - accuracy: 0.7676 - val_loss: 0.4568 - val_accuracy: 0.7824\n",
      "Epoch 11/15\n",
      "1152000/1152000 [==============================] - 1256s 1ms/step - loss: 0.4761 - accuracy: 0.7690 - val_loss: 0.4574 - val_accuracy: 0.7819\n",
      "Epoch 12/15\n",
      "1152000/1152000 [==============================] - 1276s 1ms/step - loss: 0.4757 - accuracy: 0.7693 - val_loss: 0.4556 - val_accuracy: 0.7827\n",
      "Epoch 13/15\n",
      "1152000/1152000 [==============================] - 1281s 1ms/step - loss: 0.4756 - accuracy: 0.7692 - val_loss: 0.4555 - val_accuracy: 0.7831\n",
      "Epoch 14/15\n",
      "1152000/1152000 [==============================] - 1279s 1ms/step - loss: 0.4734 - accuracy: 0.7704 - val_loss: 0.4550 - val_accuracy: 0.7836\n",
      "Epoch 15/15\n",
      "1152000/1152000 [==============================] - 1238s 1ms/step - loss: 0.4729 - accuracy: 0.7710 - val_loss: 0.4551 - val_accuracy: 0.7839\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
    "              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "history = model.fit(X_train_padded, y_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=15,\n",
    "                    validation_split=0.1,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcEzBzuFZm9c"
   },
   "outputs": [],
   "source": [
    "model.save('sentiment_analysis_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8TF-AN0wZmvI"
   },
   "outputs": [],
   "source": [
    "# Load Model\n",
    "from keras.models import load_model\n",
    "model = load_model('sentiment_analysis_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "xcRsm1lx4GNX",
    "outputId": "f85b5429-c07f-4611-9535-ccbe48f7d902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320000/320000 [==============================] - 33s 104us/step\n",
      "ACCURACY: 0.7851969003677368\n",
      "LOSS: 0.45323401675224306\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "X_test_padded = tokenizer.texts_to_sequences(X_test.text)\n",
    "X_test_padded = pad_sequences(X_test_padded, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "score = model.evaluate(X_test_padded, y_test, batch_size=512)\n",
    "print(\"ACCURACY:\",score[1])\n",
    "print(\"LOSS:\",score[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s_eE75t14F72"
   },
   "outputs": [],
   "source": [
    "def predict(text, include_neutral=True):\n",
    "    # Tokenize text\n",
    "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    # Predict\n",
    "    score = model.predict([x_test])[0]\n",
    "    if(score >=0.4 and score<=0.6):\n",
    "        label = \"Neutral\"\n",
    "    if(score <=0.4):\n",
    "        label = \"Negative\"\n",
    "    if(score >=0.6):\n",
    "        label = \"Positive\"\n",
    "\n",
    "    return {\"label\" : label,\n",
    "        \"score\": float(score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jJEMINpV5a4S",
    "outputId": "6a3176e6-3868-4e1e-eef0-6aebabc4d4b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'Positive', 'score': 0.8881966471672058}"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"It is a good product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yGgG8DJd5iah",
    "outputId": "376b59d3-99ba-43f4-a382-6c08ddc14cae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'Negative', 'score': 0.23831301927566528}"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"What the hell is this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzSbA4kd5own"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Sentiment_analysis_twitter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
